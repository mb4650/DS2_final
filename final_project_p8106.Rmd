---
title: "final_project_p8106"
author: "Michelle Lui"
date: "4/27/2021"
output: word_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(NHANES)
library(tidyverse)
library(ggplot2) 
library(caret)
library(AppliedPredictiveModeling)
library(pROC)
library(randomForest) 
library(corrplot)

```

## Import dataset

```{r}
#import dataset
data("NHANES")

#clean dataset - limit to 2011/12 and only include biological predictors, dropped some biological predictors due to excess missing data
nhanes = NHANES %>% janitor::clean_names() %>% filter(survey_yr == "2011_12") %>% select(gender, age, race1, weight, height, bmi, pulse:bp_dia3, direct_chol:urine_flow1, diabetes) %>% drop_na()
```

# Exploratory Data Analysis
```{r}
nhanes_2 = 
  nhanes %>%
  select(diabetes, everything())

summary(nhanes_2) 

#correlation plots for predictor variables
corrx = model.matrix(diabetes~., nhanes_2)[,-1]
corrplot(cor(corrx))

#remove variables with correlation greater than 0.8
indexesToDrop = findCorrelation(cor(corrx), cutoff = 0.8)
corrplot(cor(corrx[,-indexesToDrop]))


#final dataset for analysis using RF and analysis
nhanes_3 = 
  nhanes_2 %>%
  select(diabetes, age, height, bmi, pulse, bp_sys1, bp_dia3, direct_chol, tot_chol,
         urine_vol1, urine_flow1, gender, race1)


# look at feature plots for continuous predictors (everything but race1 and gender)
theme1 = transparentTheme(trans =.4)
trellis.par.set(theme1)


featurePlot(x=nhanes_3[,2:11],
            y=nhanes_3$diabetes,
            scales =list(x=list(relation ="free"),
                         y=list(relation ="free")),
            plot ="density",pch ="|",
            auto.key =list(columns =2))
```


## Create Data Partition

```{r}
set.seed(2)

train_rows = createDataPartition(y = nhanes_3$diabetes,p = 0.7,list = FALSE)
train = nhanes_3[train_rows, ]
test = nhanes_3[-train_rows, ]

x = model.matrix(diabetes ~ ., train)[ ,-1]
y = train$diabetes

x2 = model.matrix(diabetes ~ ., test)[ ,-1]
y2 = test$diabetes

control1 = trainControl(method = "cv", selectionFunction = "best", sampling = "down")
```

## Lasso Model
```{r}
set.seed(2)
lasso_fit = train(x, y, 
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, lambda = exp(seq(-20, 20,length=100))),
                   trControl = control1,
                   preProcess=c("center", "scale"),
                   family = "binomial") 

#Print the values of alpha and lambda that gave best prediction
lasso_fit$bestTune

#Print all of the options examined
lasso_fit$results

# Model coefficients
coef(lasso_fit$finalModel, s = lasso_fit$bestTune$lambda)


# Make predictions
lasso_pred = lasso_fit %>% predict(x2) %>% as.numeric()
lasso_pred_p = ifelse(lasso_pred-1 > 0.5,1,0)

test_outcome_lasso = (as.numeric(y2)-1)

misclasserror_lasso = mean(lasso_pred_p != test_outcome_lasso, na.rm=T)
print(paste('Accuracy Model 1', 1-misclasserror_lasso))

```

## Random Forest
```{r}
mtry_vals = c(ncol(train)-1, sqrt(ncol(train)-1), 0.5*ncol(train)-1)

mtry_grid = expand.grid(.mtry=mtry_vals)

set.seed(2)
rf_fit = train(diabetes ~., 
               data = train, 
               method="rf", 
               trControl = control1, 
               metric="Accuracy", 
               tuneGrid=mtry_grid, 
               ntree=100)

rf_fit$results
rf_fit$bestTune
rf_fit$finalModel

varImp(rf_fit)
plot(varImp(rf_fit))

varImpPlot(rf_fit$finalModel)

rf_pred = predict(rf_fit, test) %>% as.numeric()
rf_pred_p = ifelse(rf_pred-1 > 0.5,1,0)

test_outcome_rf = (as.numeric(test$diabetes)-1)

misclasserror_rf = mean(rf_pred_p != test_outcome_rf, na.rm=T)
print(paste('Accuracy Model 2', 1-misclasserror_rf))

```

## Model Comparisons
```{r}
resamp = resamples(list(lasso = lasso_fit, rf = rf_fit)) 
summary(resamp)
```
## Final dataset after selecting variables from RF model
We selected variables from our random forest model since it had the higher accuracy compared to the lasso model
```{r}
#Final dataset after variable selection
nhanes_4 = 
  nhanes_3 %>%
  select(diabetes, age, bmi, bp_sys1, direct_chol, urine_flow1, height, bp_dia3, pulse, urine_vol1,
         tot_chol)

#training and testing

train_rows2 = createDataPartition(y = nhanes_4$diabetes,p = 0.7,list = FALSE)
train2 = nhanes_4[train_rows, ]
test2 = nhanes_4[-train_rows, ]
```

```{r}
summary(nhanes_4)
```


## SVC Analysis
```{r}
ctrl = trainControl(method = "cv", sampling = "up",summaryFunction = twoClassSummary,
                     classProbs = TRUE)

#Analysis using linear kernal
# kernlab
set.seed(2)
svml.fit = train(diabetes ~ . , 
                  data = nhanes_4[train_rows,], 
                  method = "svmLinear",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(C = exp(seq(-5,5,len=20))),
                  trControl = ctrl)
svml.fit #C = 6.309808

plot(svml.fit, highlight = TRUE, xTrans = log)

#Analysis using radial kernal
svmr.grid = expand.grid(C = exp(seq(-4,6,len=10)),
                         sigma = exp(seq(-6,1,len=10)))

# tunes over both cost and sigma
set.seed(2)             
svmr.fit = train(diabetes ~ . , nhanes_4,
                  subset = train_rows2,
                  method = "svmRadialSigma",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)
svmr.fit #C = 1.559623, sigma = 1.248849 

plot(svmr.fit, highlight = TRUE)

#evaluate svml model
svml.pred = predict(svml.fit, newdata = test2, type = "prob")[,2]
test.pred <- rep("No", length(svml.pred))
test.pred[svml.pred>0.5] <- "Yes"

##Creating the confusion matrix:
confusionMatrix(data = as.factor(test.pred),
                reference = test2$diabetes,
                positive = "Yes")

#evaluate svmr model
svmr.pred = predict(svmr.fit, newdata = test2, type = "prob")[,2]
test.pred <- rep("No", length(svmr.pred))
test.pred[svmr.pred>0.5] <- "Yes"

##Creating the confusion matrix:
confusionMatrix(data = as.factor(test.pred),
                reference = test2$diabetes,
                positive = "Yes")
```

## QDA
```{r}
#qda
set.seed(2)

ctrl1 = trainControl(method = "cv",sampling = "up",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

model.qda = train(diabetes~., data = train2,
                  method = "qda",
                  preProcess = c("center","scale"),
                  trControl = ctrl1)

#evaluate qda model
qda.pred = predict(model.qda, newdata = test2, type = "prob")[,2]
test.pred <- rep("No", length(qda.pred))
test.pred[qda.pred>0.5] <- "Yes"

##Creating the confusion matrix:
confusionMatrix(data = as.factor(test.pred),
                reference = test2$diabetes,
                positive = "Yes")
```

## MARS

```{r}


library(MASS)
library(earth)

set.seed(2)

mars_grid <- expand.grid(degree = 1:5, 
                         nprune = 2:20)
mars.fit <- train(diabetes ~., data = train2,
                 method = "earth",
                 tuneGrid = mars_grid,
                 trControl = ctrl)

#evaluate MARS model
test.pred.prob.mars <- predict(mars.fit, newdata = test2, type = "prob")[,2]
test.pred <- rep("No", length(test.pred.prob.mars))
test.pred[test.pred.prob.mars>0.5] <- "Yes"

##Creating the confusion matrix:
confusionMatrix(data = as.factor(test.pred),
                reference = test2$diabetes,
                positive = "Yes")

```


```{r}
library(vip)
vip(mars.fit, num_features = 17, bar = FALSE, value = "gcv") + ggtitle("Variable importance")
```


Accuracy : 0.7521


## KNN

```{r}
set.seed(2)
knn.fit <- train(diabetes ~., data = train2,
                   method = "knn",
                   preProcess = c("center","scale"),
                   tuneGrid = data.frame(k = seq(2,200,by=5)),
                   trControl = ctrl)

ggplot(knn.fit, highlight = TRUE)

test.pred.prob.knn <- predict(knn.fit, newdata = test2,
                           type = "prob")[,2]
test.pred <- rep("No", length(test.pred.prob.knn))
test.pred[test.pred.prob.knn>0.5] <- "Yes"

##Creating the confusion matrix:
confusionMatrix(data = as.factor(test.pred),
                reference = test2$diabetes,
                positive = "Yes")

```

Accuracy : 0.7331


##LDA
```{r}
set.seed(2)
lda.fit <- train(diabetes ~., data = train2, preProcess = c("center","scale"),
                   method = "lda",
                   trControl = ctrl3)

test.pred.prob.lda <- predict(lda.fit, newdata = test2, type = "prob")[,2]
test.pred <- rep("No", length(test.pred.prob.lda))
test.pred[test.pred.prob.lda>0.5] <- "Yes"

##Creating the confusion matrix:
confusionMatrix(data = as.factor(test.pred),
                reference = test2$diabetes,
                positive = "Yes")

```

Accuracy : 0.6952

##Plotting the ROC curves to determine the best model, printing the AUC. 
```{r}
set.seed(2)

roc.svml <- roc(test2$diabetes, svml.pred)
roc.svmr <- roc(test2$diabetes, svmr.pred)
roc.qda <- roc(test2$diabetes, qda.pred)
roc.MARS <- roc(test2$diabetes, test.pred.prob.mars)
roc.lda <- roc(test2$diabetes, test.pred.prob.lda)
roc.knn <- roc(test2$diabetes, test.pred.prob.knn)


auc <- c(roc.svml$auc[1],roc.svmr$auc[1],roc.qda$auc[1], roc.MARS$auc[1], roc.lda$auc[1], roc.knn$auc[1])

plot(roc.svml, legacy.axes = TRUE, col = 1)
plot(roc.qda, legacy.axes = TRUE, add=TRUE, col = 2)
plot(roc.MARS, legacy.axes = TRUE, add=TRUE, col = 3)
plot(roc.lda, legacy.axes = TRUE, add=TRUE, col = 4)
plot(roc.knn, legacy.axes = TRUE, add=TRUE, col = 5)
plot(roc.svmr, legacy.axes = TRUE, add=TRUE, col = 6)
models.used <- c("SVML","QDA","MARS", "LDA", "KNN", "SVMR") 

legend("bottomright", legend = paste0(models.used, ": ", round(auc,3)), col = 1:6, lwd = 1)


```


## Model Comparisons
```{r}
resamp2 = resamples(list(svml = svml.fit, svmr = svmr.fit, qda = model.qda, mars = mars.fit, knn = knn.fit, lda = lda.fit)) 
summary(resamp2)
```